{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Manual+EDA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjDo1XVyu5Vr",
        "colab_type": "text"
      },
      "source": [
        "### **MIDAS SCREENING TASK 1: R/INDIA FLAIR DETECTOR**\n",
        "\n",
        "This is a manual/documentation of the various attempts at succeding in the screening task provided by IIITD for MIDAS research labs by Anant Raina, a student of Nirma University\n",
        "\n",
        "## Task 1: REDDIT DATA COLLECTION\n",
        "\n",
        "The first task is to get the data from the r/India Subreddit.<br>\n",
        "\n",
        "We can use the PRAW Python Reddit API Wrapper. Docs: [link text](https://praw.readthedocs.io/en/latest/)<br>\n",
        "\n",
        "PRAW is a free to use Python Reddit API wrapper that will help us to get the data from Subreddits. The following, however, needs to be done before proceeding further, as per the documentation:\n",
        "\n",
        "Step 1: Go to [link text](https://www.reddit.com/prefs/apps) and create new app<br>\n",
        "Step 2: Fill out the form like so:\n",
        "\n",
        "\n",
        "*   Name: \"Name Of App\"\n",
        "*   App Type: Script\n",
        "*   description: You can leave this blank\n",
        "*   about url: You can leave this blank\n",
        "*   redirect url: http://www.\"Name Of App\".com/unused/redirect/uri\n",
        "\n",
        "Step 3: Hit create app. Make note of the client ID and the client secret.\n",
        "\n",
        "After this we may need to install praw. A simple way would be to use pip."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAnNPcIQU2D4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install praw"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CF1Tth_lVDpM",
        "colab_type": "text"
      },
      "source": [
        "Let us try to get a single reddit post from r/India"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BTGIHmPVO40",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b3afaa53-ced0-451f-8d1c-59ec85d5a6a8"
      },
      "source": [
        "import praw\n",
        "\n",
        "reddit = praw.Reddit(\n",
        "\t\t\t\t\t\tclient_id=\"RImabBQtiUpnDw\",\n",
        "\t\t\t\t\t\tclient_secret=\"VOhkZ1p8g215x4hY354QEKdUEn0\",\n",
        "\t\t\t\t\t\tuser_agent=\"python-linux:text_classifier:v0.1a (by u/reddit_scraper_)\"\n",
        "\t\t\t\t\t)\n",
        "\n",
        "for submission in reddit.subreddit('india').hot(limit=1):\n",
        "  print(submission.title, submission.link_flair_text)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Coronavirus (COVID-19) Megathread - News and Updates - 4 Coronavirus\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKJNSqkfVzK4",
        "colab_type": "text"
      },
      "source": [
        "If we actually go to r/India and see the first post, this post will show up with the obtained flair. This is because this post is pinned to the top and hence shows up first when we try and get the data. Let us try to get more submissions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMQbcb_wWGTP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for submission in reddit.subreddit('india').hot(limit=50):\n",
        "  print(submission.title, submission.link_flair_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7iJ-j-dWQxj",
        "colab_type": "text"
      },
      "source": [
        "It is clear that the reddit API is working just fine. It is now time to decide on what to scrape, how much to scrape, and how to preprocess it. Let us now get to task 2. For more on reddit_scraping, see reddit_scraping.ipynb in the repository under jupyter notebooks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHZ9gnIcWmXr",
        "colab_type": "text"
      },
      "source": [
        "## **Task 2: EDA**\n",
        "\n",
        "The next task is to perform an analysis on what data to get, and how much to get.<br>\n",
        "# What to get\n",
        "\n",
        "We first need to decide what is required and what is not. The following can be scraped from a reddit post:\n",
        "\n",
        "*   URL\n",
        "*   Post Title\n",
        "*   Post Body\n",
        "*   Comments\n",
        "*   Post Flair\n",
        "\n",
        "We need to have a train_dataset and a train_labelset. train_labels can be obtained from scraping for the reddit flair. So Flair needs to be obtained. What is left is the train_data. For train_data the easiest to train and obtain is the URL and the title. <br>\n",
        "\n",
        "From a Machine Learning perspective, intuitively title seems like the best way to train a classifier. The following reasons can also be cited for the same:\n",
        "\n",
        "\n",
        "1.   It is a short string. It does not require a lot of pre processing\n",
        "2.   It is uniform across all the posts. There will be only unicode information in all titles and no images or sounds or links can find its way into the title\n",
        "\n",
        "Let us look at the trends in the three sorting types: New, Hot, Top\n",
        "\n",
        "# HOT\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=14RmlK8b3d3RtdRfPFUfrUtrBJlmOUyzV)\n",
        "\n",
        "It is clear that Coronavirus is very trending at the moment. And we must keep this in mind when scraping data as it may bias the dataset. The only other tags that are prevelant is Politics and Non-Political. And AskIndia becomes prevelant when limit is set to None and is free to obtain as many submissions as possible.\n",
        "\n",
        "# NEW\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1Mflw2dV5QW5SYT-brwKlZ0el-xCymAuk)\n",
        "\n",
        "Coronavirus breaks the plot here. This shows how popular the talking points about the virus are. This can cause problems for us. There are similar observations to HOT.\n",
        "\n",
        "# TOP\n",
        "\n",
        "Sorting by top will not help. The following reasons are observed for it.\n",
        "\n",
        "*   Coronavirus flair is dominant on r/India and TOP posts of all time will most generally show flairs that were prevelant(dominant) before such as demonetization, GST that are not prevelant now and so it is not advisable to run with TOP\n",
        "*   r/India suffers from no Automoderators that keep track of flairs and no moderators seem to moderate flairs: oftentimes new flairs that do not have enough/have any posts in it such as \"OC\", \"Unverified\", \"| REPOST |\" etc. which should be avoided. TOP has a lot of such flairs that are either not Canon anymore or simply too unused to be added as a parameter.\n",
        "\n",
        "So what can be used as a parameter for a useful flair? Here are the key parameters:\n",
        "\n",
        "*   It needs to be relevent in the current times\n",
        "*   It needs to have enough posts and have some form of validation so as to its selection.\n",
        "\n",
        "A good way would be to start a new post on r/India and select a flair for the post. Here are the listed flairs:\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1exsXh3X7YV9r7pAju3ab-mgdCeFZDOLd)\n",
        "\n",
        "Interesting.<br>\n",
        "\n",
        "This can be used as our train_label flairs perhaps?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFZqHqIWgIfN",
        "colab_type": "text"
      },
      "source": [
        "### **PART 3: MAKE A FLAIR DETECTOR**\n",
        "\n",
        "Let us now try to build a classifier, now that we have train_data and train_labels. To refer to code that converts the EDA to a training dataset one can refer to reddit_scraping.ipynb in Jupyter Notebooks in the repository.\n",
        "\n",
        "## Make the model.\n",
        "\n",
        "It feels that the string-based nature of the data pushes a need for Machine Learning to step in. The first attempt at a classifier was a OneVsRest Classifier. The model looks like so:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYpnB2iNWjv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lb = preprocessing.MultiLabelBinarizer()\n",
        "Y = lb.fit_transform(y_train_text)\n",
        "\n",
        "classifier = Pipeline([\n",
        "\t('vectorizer', CountVectorizer()),\n",
        "\t('tfidf', TfidfTransformer()),\n",
        "\t('clf', OneVsRestClassifier(LinearSVC()))])\n",
        "\n",
        "classifier.fit(X_train, Y)\n",
        "predicted = classifier.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9kEgSZWhI9V",
        "colab_type": "text"
      },
      "source": [
        "There were, of course, some issues with using the OneVsRest Classifier.\n",
        "It had high accuracy (74%) but it oftentimes output no output, no classification, which after days of debugging led to no explanation. Also, at this point in time the data was placed without any preprocessing and hence often was broken by inverted commas and quotes and commas. Hence there was a need for clearning the data. The following code dependant on the NLP library \"nltk\" and \"re\" helps us here. Code is as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUw0icHeh30F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Get all the symbols that are undesired and unwanted in the training/testing set\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+,_\\\"]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "TAG_RE = re.compile(r'<[^>]+>')\n",
        "\n",
        "# Remove HTML tags if any from the samples\n",
        "def remove_tags(text):\n",
        "    return TAG_RE.sub('', text)\n",
        "\n",
        "# Perform removal of unwanted symbols from the samples\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = remove_tags(text)\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
        "    text = BAD_SYMBOLS_RE.sub('', text)\n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTny79_Sh-pk",
        "colab_type": "text"
      },
      "source": [
        "This will remove all the unwanted symbols in BAD_SYMBOLS_RE and REPLACE_BY_SYMBOLS_RE and also any stopwords in the english language that follows its way into the title, or any html tags will be filtered out of the test string.\n",
        "\n",
        "The classifier still failed, with the accuracy only reaching 44% for 1722 new posts. The problem still persisted, with blank string predictions reaching its way into the answer repeatedly.\n",
        "\n",
        "We needed new classifiers, perhaps a trial and error on all possibilities and then choose the best one.\n",
        "\n",
        "So I developed new models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MfYno0Qi7Tb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ml_train(model_type):\n",
        "\t\n",
        "\tif model_type == \"baye\":\n",
        "\t\tprint(y_train)\n",
        "\t\tclassifier = Pipeline([\n",
        "\t\t\t('vectorizer', CountVectorizer()),\n",
        "\t\t\t('tfidf', TfidfTransformer()),\n",
        "\t\t\t('clf', MultinomialNB())])\n",
        "\t\tclassifier.fit(X_train, y_train_vectored)\n",
        "\t\tfilename = 'native_baye.sav'\n",
        "\t\tjoblib.dump(classifier, filename)\n",
        "\n",
        "\telif model_type == \"sgd\":\n",
        "\t\tprint(y_train)\n",
        "\t\tclassifier = Pipeline([\n",
        "\t\t\t('vectorizer', CountVectorizer()),\n",
        "\t\t\t('tfidf', TfidfTransformer()),\n",
        "\t\t\t('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None))])\n",
        "\t\tclassifier.fit(X_train, y_train_vectored)\n",
        "\t\tfilename = 'sgd.sav'\n",
        "\t\tjoblib.dump(classifier, filename)\n",
        "\n",
        "\telif model_type == \"regression\":\n",
        "\t\tprint(y_train)\n",
        "\t\tclassifier = Pipeline([\n",
        "\t\t\t('vectorizer', CountVectorizer()),\n",
        "\t\t\t('tfidf', TfidfTransformer()),\n",
        "\t\t\t('clf', LogisticRegression(n_jobs=1, C=1e5, max_iter=10000))])\n",
        "\t\tclassifier.fit(X_train, y_train_vectored)\n",
        "\t\tfilename = 'regression.sav'\n",
        "\t\tjoblib.dump(classifier, filename)\n",
        "\n",
        "\telif model_type == \"random_forrest\":\n",
        "\t\tprint(y_train)\n",
        "\t\tclassifier = Pipeline([\n",
        "\t\t\t('vectorizer', CountVectorizer()),\n",
        "\t\t\t('tfidf', TfidfTransformer()),\n",
        "\t\t\t('clf', RandomForestClassifier(n_estimators = 1000, random_state = 42))])\n",
        "\t\tclassifier.fit(X_train, y_train_vectored)\n",
        "\t\tfilename = 'random_forrest.sav'\n",
        "\t\tjoblib.dump(classifier, filename)\n",
        "\n",
        "\telif model_type == \"all\":\n",
        "\t\tml_train(\"baye\")\n",
        "\t\tml_train(\"sgd\")\n",
        "\t\tml_train(\"regression\")\n",
        "\t\tml_train(\"random_forrest\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWcAAP3RjAim",
        "colab_type": "text"
      },
      "source": [
        "This seemed to work: the problem of blank prediction was solved and we had the opportunity to choose the best from a plethora of models. The accuracy reached now was as follows:\n",
        "\n",
        "*On Title*<br>\n",
        "NB: 53%<br>\",\n",
        "LogRegression: 49%<br>\n",
        "RandomForrest: 46%<br>\n",
        "LinearSVM: 49%<br>\n",
        "\n",
        "This seems to make NB the best, however we are missing one thing: there are other ways to train the classifier. Such as comments and the url itself. The scraping for these datasets can be seen in reddit_scraping.ipynb. Here are the results of the same:\n",
        "\n",
        "*On Comments*<br>\n",
        "NB: 26%<br>\n",
        "LogRegression: 22%<br>\n",
        "RandomForrest: 24%<br>\n",
        "LinearSVM: 22%<br>\n",
        "\n",
        "*On URL*<br>\n",
        "NB: 10%<br>\n",
        "LogRegression: 8%<br>\n",
        "RandomForrest: 7%<br>\n",
        "LinearSVM: 8%<br>\n",
        "\n",
        "So that was a trainwreck but we need to figure out why. For URL we can answer intuitively but for comments we need to delve deeper. Here are some noticable things:\n",
        "\n",
        "1) Comments are subjective and may lead to tangents. \n",
        "2) Comments on r/India are often hindi words worded in english, which may throw off the classifier.\n",
        "\n",
        "Let us now look at why title fails. Let us look at this title that was found while training.\n",
        "\n",
        "\"I just got fired\"\n",
        "\n",
        "Without the context, it appears to be Non-Political. But it is flaired as Coronavirus....this is because the person making the post has lost his job due to the outbreak. Hence the confusion of the classifier. \n",
        "\n",
        "That was my journey through this problem set. Thank you."
      ]
    }
  ]
}