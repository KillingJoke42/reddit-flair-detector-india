{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reddit_scraping.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nB6qbA14sZ0q",
        "colab_type": "text"
      },
      "source": [
        "**REDDIT SCRAPING: GET DATA VIA THE PYTHON REDDIT API WRAPPER**<BR>\n",
        "This code snippet is tasked with getting the title/comments/urls from the reddit posts in order to prepare a training set.\n",
        "\n",
        "Limit has been set to None so as to obtain as many training samples as the PRAW Wrapper can fetch. <br>\n",
        "A throwaway reddit username r/reddit_scraper_ has been created with a blank slate: no user information as well as only one subbed subreddit: r/India. This username can be used for research into the code. Throwaway username will be removed post-screening task after which the user must use his/her own username and password/ client id and secret than the one provided in-code<br><br>\n",
        "\n",
        "Separate code blocks for each segment have been created so as to make playing with the code in jupyter playground easier and isolated from one another"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ui8QRfVatTTp",
        "colab_type": "text"
      },
      "source": [
        "**GET POST TITLE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ddEM2pfrlHy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get Python's Reddit API Wrapper\n",
        "import praw\n",
        "# Pickle will store generated data for EDA\n",
        "import pickle\n",
        "\n",
        "# All the flairs that were consistent with current affairs on r/India\n",
        "link_flair_tags = {\n",
        "\t\t\t\t\t\"Politics\":0, \n",
        "\t\t\t\t\t\"Non-Political\":0, \n",
        "\t\t\t\t\t\"AskIndia\":0, \n",
        "\t\t\t\t\t\"Policy/Economy\":0, \n",
        "\t\t\t\t\t\"Business/Finance\":0, \n",
        "\t\t\t\t\t\"Science/Technology\":0, \n",
        "\t\t\t\t\t\"Scheduled\":0, \n",
        "\t\t\t\t\t\"Sports\":0, \n",
        "\t\t\t\t\t\"Food\":0,\n",
        "\t\t\t\t\t\"Photography\":0,\n",
        "\t\t\t\t\t\"CAA-NRC-NPR\":0,\n",
        "\t\t\t\t\t\"Coronavirus\":0,\n",
        "\t\t\t\t  }\n",
        "\n",
        "# We need to store both data collected and tbe statistics: hence we require file pointers\n",
        "dataset_file = open(\"hot/flair_dataset.csv\", \"w\")\n",
        "data_analysis = open(\"hot/data_analysis.pickle\", \"wb\")\n",
        "\n",
        "# Generate a Reddit API Wrapper object: PRAW object to scrape reddit data\n",
        "reddit = praw.Reddit(\n",
        "\t\t\t\t\t\tclient_id=\"RImabBQtiUpnDw\",\n",
        "\t\t\t\t\t\tclient_secret=\"VOhkZ1p8g215x4hY354QEKdUEn0\",\n",
        "\t\t\t\t\t\tuser_agent=\"python-linux:text_classifier:v0.1a (by u/reddit_scraper_)\"\n",
        "\t\t\t\t\t)\n",
        "\n",
        "# Counter to keep track of number of posts that were scraped\n",
        "count = 0\n",
        "\n",
        "# Get posts from the required sub with appropriate sorting bias\n",
        "for submission in reddit.subreddit('india').hot(limit=None):\n",
        "\t# Internal parameters upated\n",
        "\tcount += 1\n",
        "\ttitle = \"\"\n",
        "\tflair = \"\"\n",
        "\tflag = False\n",
        "\n",
        "\t# Check whether the received request is valid or not\n",
        "\tif submission.link_flair_text:\n",
        "\t\t# If the received flair is not part of the shortlisted flairs, Ignore it\n",
        "\t\ttry:\n",
        "\t\t\t# Coronavirus posts are overflowing.... to control bias we ignore posts over 150.\n",
        "\t\t\tif submission.link_flair_text == \"Coronavirus\" and link_flair_tags[submission.link_flair_text] > 150:\n",
        "\t\t\t\tprint('Corona Exceeded')\n",
        "\t\t\t# Counting number of posts under the flair\n",
        "\t\t\telse:\n",
        "\t\t\t\tlink_flair_tags[submission.link_flair_text] += 1\n",
        "\t\t\t\tflair = submission.link_flair_text\n",
        "\t\t\t\tflag = True\n",
        "\t\texcept KeyError:\n",
        "\t\t\tprint(\"New Flair Found: \" + submission.link_flair_text + \". Ignoring..........\")\n",
        "\t\t\tflag = False\n",
        "\n",
        "\t# Get the title of received post\n",
        "\tif submission.title:\n",
        "\t\ttitle = submission.title\n",
        "\t\tif ',' in title:\n",
        "\t\t\ttitle = '\\\"' + title + '\\\"'\n",
        "\t# Data that is generated needs to be from a shortlisted flair: else it is not included in statistics\n",
        "\tif flag == True:\n",
        "\t\tdataset_file.write(title + ',' + flair + '\\n')\n",
        "\n",
        "# Validation of code working correctly: get the statistics scraped by the bot\n",
        "print(link_flair_tags)\n",
        "print(count)\n",
        "\n",
        "# Close all file pointers\n",
        "dataset_file.close()\n",
        "data_analysis.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lw1oBG5WtbAq",
        "colab_type": "text"
      },
      "source": [
        "**GET POST COMMENTS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqH0J-x6tan-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import praw\n",
        "import pickle\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+,_\\\"]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
        "    text = BAD_SYMBOLS_RE.sub('', text)\n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
        "    return text\n",
        "\n",
        "link_flair_tags = {\n",
        "\t\t\t\t\t\"Politics\":0, \n",
        "\t\t\t\t\t\"Non-Political\":0, \n",
        "\t\t\t\t\t\"AskIndia\":0, \n",
        "\t\t\t\t\t\"Policy/Economy\":0, \n",
        "\t\t\t\t\t\"Business/Finance\":0, \n",
        "\t\t\t\t\t\"Science/Technology\":0, \n",
        "\t\t\t\t\t\"Scheduled\":0, \n",
        "\t\t\t\t\t\"Sports\":0, \n",
        "\t\t\t\t\t\"Food\":0,\n",
        "\t\t\t\t\t\"Photography\":0,\n",
        "\t\t\t\t\t\"CAA-NRC-NPR\":0,\n",
        "\t\t\t\t\t\"Coronavirus\":0,\n",
        "\t\t\t\t  }\n",
        "\n",
        "dataset_file = open(\"new/flair_dataset_comments.csv\", \"w\")\n",
        "data_analysis = open(\"new/data_analysis_comments.pickle\", \"wb\")\n",
        "\n",
        "reddit = praw.Reddit(\n",
        "\t\t\t\t\t\tclient_id=\"RImabBQtiUpnDw\",\n",
        "\t\t\t\t\t\tclient_secret=\"VOhkZ1p8g215x4hY354QEKdUEn0\",\n",
        "\t\t\t\t\t\tuser_agent=\"python-linux:text_classifier:v0.1a (by u/reddit_scraper_)\"\n",
        "\t\t\t\t\t)\n",
        "\n",
        "count = 0\n",
        "\n",
        "for submission in reddit.subreddit('india').new(limit=None):\n",
        "\tcount += 1\n",
        "\tcomments = \"\"\n",
        "\tflair = \"\"\n",
        "\tflag = False\n",
        "\tif submission.link_flair_text:\n",
        "\t\ttry:\n",
        "\t\t\tif submission.link_flair_text == \"Coronavirus\" and link_flair_tags[submission.link_flair_text] > 150:\n",
        "\t\t\t\tprint('Corona Exceeded')\n",
        "\t\t\telse:\n",
        "\t\t\t\tlink_flair_tags[submission.link_flair_text] += 1\n",
        "\t\t\t\tflair = submission.link_flair_text\n",
        "\t\t\t\tflag = True\n",
        "\t\texcept KeyError:\n",
        "\t\t\tprint(\"New Flair Found: \" + submission.link_flair_text + \". Ignoring..........\")\n",
        "\t\t\t#link_flair_tags[submission.link_flair_text] = 1\n",
        "\t\t\t#flair = submission.link_flair_text\n",
        "\t\t\tflag = False\n",
        "\n",
        "\tif submission.comments:\n",
        "\t\tfor top_level_comment in submission.comments:\n",
        "\t\t\tif isinstance(top_level_comment, praw.models.MoreComments):\n",
        "\t\t\t\tcontinue\n",
        "\t\t\tcomments = comments + ' ' + top_level_comment.body\n",
        "\t\t\tcomments = preprocess_text(comments)\n",
        "\tif flag == True:\n",
        "\t\tdataset_file.write(comments + ',' + flair + '\\n')\n",
        "\n",
        "#pickle.dump(link_flair_tags, data_analysis)\n",
        "print(link_flair_tags)\n",
        "print(count)\n",
        "\n",
        "dataset_file.close()\n",
        "data_analysis.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}